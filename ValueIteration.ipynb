{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 83.\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./Figures/ValueIteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration, for estimating Ï€\n",
    "\n",
    "**Value Iteration**\n",
    "\n",
    "Given a **policy**, one finds the associated **values** using the **iterative policy evaluation** algorithm. Given **values**, one can find the associated **policy**.  Iterating policy evaluation and finding a policy until the policy does not change is the **policy iteration** algorithm.\n",
    "\n",
    "**Value iteration** proceeds by interleaving value calculations with policy updates.  Convergence occurs when the values do not change.\n",
    "\n",
    "Note that for this algorithm one does not need an initial policy.\n",
    "\n",
    "**Value Iteration Algorithm**\n",
    "```python\n",
    "initialize values (to zero, or randomly)\n",
    "while not converged:\n",
    "    for each state \n",
    "        for each decision (at each state)\n",
    "             value = max(reward + gamma*value at dest)\n",
    "compute policy from values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "from rlgridworld.algorithms import compute_policy_from_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from page 83 of Sutton and Barto, RL 2nd. Ed.\n",
    "def value_iteration(gw, gamma=0.9, epsilon=0.001):\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change_in_value = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                old_value = gw.get_value(state)\n",
    "                new_value = float('-inf')\n",
    "                # valid decisions and rewards at current state\n",
    "                dr = gw.valid_decisions_and_rewards(state)\n",
    "                for action, reward in dr.items():\n",
    "                    reward = gw.get_reward_for_action(state, action)\n",
    "                    value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                    value = reward + gamma*value_at_dest\n",
    "                    if value > new_value:\n",
    "                        new_value = value\n",
    "                    gw.set_value(state, new_value)\n",
    "                biggest_change_in_value = max(biggest_change_in_value,\n",
    "                                                  abs(new_value - old_value))\n",
    "        if biggest_change_in_value < epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Values\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "\n",
      "Values after Value Iteration\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |   0.90 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.66 |   0.73 |   0.81 |   0.73 |\n",
      "-------------------------------------\n",
      "\n",
      "New Policy\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |     Up |        |\n",
      "-------------------------------------\n",
      "|  Right |  Right |     Up |   Left |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gw = create_standard_grid()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute values\n",
    "value_iteration(gw)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Values after Value Iteration\")\n",
    "gw.print_values()\n",
    "\n",
    "# compute policy from values\n",
    "policy = compute_policy_from_values(gw)\n",
    "\n",
    "print(\"\") \n",
    "print(\"New Policy\")\n",
    "gw.print_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
