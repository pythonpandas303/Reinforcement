{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sutton and Barto, Reinforcement Learning 2nd. Edition, page 75.\n",
    "![Sutton and Barto, Reinforcement Learning 2nd. Edition.](./Figures/IterativePolicyEvaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Iterative Policy Evaluation, for estimating V\n",
    "\n",
    "**Iterative Policy Evaluation**\n",
    "\n",
    "A **policy** must say what to do at every state. The idea of iterative policy evaluation is to use this policy to compute values.  That is, loop over all states and compute a value for all of them using the given policy.  \n",
    "\n",
    "Repeat this calculation, over all state, until the values do not change.\n",
    "\n",
    "**Iterative Policy Evaluation Algorithm**\n",
    "``` python\n",
    "Given a policy\n",
    "while not converged:\n",
    "    # compute at each state compute new value until values do not change\n",
    "    value = reward + gamma*value at dest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create standard grid world problem and its state space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlgridworld.standard_grid import create_standard_grid\n",
    "gw = create_standard_grid()\n",
    "\n",
    "gw.set_reward((0,0), \"up\", -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify a policy function for the state space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = { \n",
    "    (0,0):'up', (0,1):'right',(0,2):'right',(0,3):'up',\n",
    "    (1,0):'up', (1,1):'', (1,2):'right', (1,3):'',\n",
    "    (2,0):'right', (2,1):'right', (2,2):'right', (2,3):''\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-29T21:54:11.507Z",
     "iopub.status.busy": "2021-04-29T21:54:11.499Z",
     "iopub.status.idle": "2021-04-29T21:54:11.616Z",
     "shell.execute_reply": "2021-04-29T21:54:11.628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "Initial Values\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.00 |   0.00 |   0.00 |   0.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy\")\n",
    "gw.print_policy(policy)\n",
    "print(\"Initial Values\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the implementation of the iterative policy evaluation algorithm.  It loops over all states.  It does no computations at barrier or terminal states.  \n",
    "\n",
    "The code then retrieves information from the policy and grid and performs the key computation\n",
    "```python\n",
    "# compute at each state compute new value until values do not change\n",
    "value = reward + gamma*value_at_dest\n",
    "```\n",
    "at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-29T21:54:17.461Z",
     "iopub.status.busy": "2021-04-29T21:54:17.449Z",
     "iopub.status.idle": "2021-04-29T21:54:17.478Z",
     "shell.execute_reply": "2021-04-29T21:54:17.491Z"
    }
   },
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(gw, policy, gamma=0.9, theta=0.001):\n",
    "    \n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for node in gw:\n",
    "            state = node.state\n",
    "            if not gw.is_terminal(state) and not gw.is_barrier(state):\n",
    "                # get current (old) value\n",
    "                old_value = gw.get_value(state)\n",
    "                # get action from policy\n",
    "                action = policy[state]\n",
    "                # get immediate reward for action\n",
    "                reward = gw.get_reward_for_action(state, action)\n",
    "                # get value at destination state\n",
    "                value_at_dest = gw.get_value_at_destination(state, action)\n",
    "                # compute new value\n",
    "                new_value = reward + gamma*value_at_dest\n",
    "                # set new value for state\n",
    "                gw.set_value(state, new_value)\n",
    "                # see if |new_value-old_value| is larger than biggest_change\n",
    "                biggest_change = max(\n",
    "                    biggest_change, abs(new_value-old_value))\n",
    "        # iterated over all states, so see if biggest_change is small enough\n",
    "        if biggest_change < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-29T21:54:21.747Z",
     "iopub.status.busy": "2021-04-29T21:54:21.739Z",
     "iopub.status.idle": "2021-04-29T21:54:21.764Z",
     "shell.execute_reply": "2021-04-29T21:54:21.770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "-------------------------------------\n",
      "|  Right |  Right |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |        |  Right |        |\n",
      "-------------------------------------\n",
      "|     Up |  Right |  Right |     Up |\n",
      "-------------------------------------\n",
      "Values for the policy\n",
      "-------------------------------------\n",
      "|   0.81 |   0.90 |   1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|   0.73 |   0.00 |  -1.00 |   0.00 |\n",
      "-------------------------------------\n",
      "|  -1.34 |  -0.81 |  -0.90 |  -1.00 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy\")\n",
    "gw.print_policy(policy)\n",
    "iterative_policy_evaluation(gw, policy, gamma = 0.9)\n",
    "print(\"Values for the policy\")\n",
    "gw.print_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
